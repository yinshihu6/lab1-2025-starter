{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analyzing Deep Neural Networks\n",
    "In this notebook, we will recreate the functions that PyTorch provides for the\n",
    "individual layers in our network using primtive Python code. The goal of this\n",
    "notebook is to understand what happens in a network underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from loaders import *\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import inspect\n",
    "from math import prod\n",
    "\n",
    "%matplotlib inline\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing PyTorch functions\n",
    "First, we will implement the functions that PyTorch provides for the individual\n",
    "layers in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "For answer, expected an answer. Please fill in the answer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m flatten_test_y \u001b[38;5;241m=\u001b[39m flatten(flatten_test_x)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(flatten_test_y, flatten_test_y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlatten function is incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 60\u001b[0m \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is your implementation of the conv function?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFILL ME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[0;32m     65\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m answer(\n\u001b[0;32m     67\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m     subquestion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your implementation of the fc function?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     69\u001b[0m     answer\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILL ME\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     70\u001b[0m     required_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m answer(\n\u001b[0;32m     73\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     74\u001b[0m     subquestion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your implementation of the relu function?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m     answer\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILL ME\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     76\u001b[0m     required_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     77\u001b[0m )\n",
      "File \u001b[1;32m~\\ENGN2911U\\lab1-2025-starter\\workspace\\lab1\\loaders.py:101\u001b[0m, in \u001b[0;36manswer\u001b[1;34m(question, subquestion, answer, required_type, assumptions)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer\u001b[39m(\n\u001b[0;32m     95\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     96\u001b[0m     subquestion: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     assumptions: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    100\u001b[0m ):\n\u001b[1;32m--> 101\u001b[0m     \u001b[43mcheck_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m assumptions:\n\u001b[0;32m    104\u001b[0m         assumptions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\ENGN2911U\\lab1-2025-starter\\workspace\\lab1\\loaders.py:77\u001b[0m, in \u001b[0;36mcheck_type\u001b[1;34m(context, a, t)\u001b[0m\n\u001b[0;32m     74\u001b[0m         check_type(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, ai, ti)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m---> 77\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m a \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFILL ME\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected an answer. Please fill in the answer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     80\u001b[0m         t \u001b[38;5;241m=\u001b[39m (t,)\n",
      "\u001b[1;31mAssertionError\u001b[0m: For answer, expected an answer. Please fill in the answer."
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def conv(x, W, b):\n",
    "    # DO NOT USE ANY LIBRARY CONVOLUTION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    if np.ndim(x) == 2:\n",
    "        x = np.expand_dims(x, axis = 0)\n",
    "    assert np.ndim(x) == 3\n",
    "    C_in, M, N = x.shape\n",
    "    C_out, C_t, Kx, Ky = W.shape\n",
    "    assert C_in == C_t, \"activation and weight dimension mismatch!\"\n",
    "    # Assume this padding\n",
    "    x_padded = np.pad(x,((0,0),(Kx//2,Kx//2),(Ky//2,Ky//2)))\n",
    "\n",
    "    # Your code here\n",
    "    return np.array([])\n",
    "    \n",
    "conv_test_x = np.random.rand(3, 5, 5)\n",
    "conv_test_W = np.random.rand(2, 3, 3, 3)\n",
    "conv_test_b = np.random.rand(2)\n",
    "conv_test_y = conv(conv_test_x, conv_test_W, conv_test_b)\n",
    "assert np.allclose(conv_test_y, conv_test_y), \"Convolution function is incorrect\"\n",
    "    \n",
    "\n",
    "def fc(x, W, b):\n",
    "    # DO NOT USE ANY LIBRARY MATRIX MULTIPLICATION FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    # Your code here\n",
    "    return np.array([])\n",
    "\n",
    "fc_test_x = np.random.rand(5)\n",
    "fc_test_W = np.random.rand(3, 5)\n",
    "fc_test_b = np.random.rand(3)\n",
    "fc_test_y = fc(fc_test_x, fc_test_W, fc_test_b)\n",
    "assert np.allclose(fc_test_y, fc_test_y), \"Fully connected function is incorrect\"\n",
    "\n",
    "def relu(x):\n",
    "    # Your code here\n",
    "    return np.array([])\n",
    "    \n",
    "relu_test_x = np.random.rand(5, 5)\n",
    "relu_test_y = relu(relu_test_x)\n",
    "assert np.allclose(relu_test_y, relu_test_y), \"ReLU function is incorrect\"\n",
    "\n",
    "def pool2(x, dh, dw):\n",
    "    # DO NOT USE ANY LIBRARY POOLING FUNCTIONS. WRITE YOUR OWN LOOP NEST.\n",
    "    # Your code here\n",
    "    return np.array([])\n",
    "    \n",
    "pool2_test_x = np.random.rand(3, 4, 4)\n",
    "pool2_test_y = pool2(pool2_test_x, 2, 2)\n",
    "assert np.allclose(pool2_test_y, pool2_test_y), \"Pooling function is incorrect\"\n",
    "\n",
    "def flatten(x):\n",
    "    # Your code here\n",
    "    return np.array([])\n",
    "    \n",
    "flatten_test_x = np.random.rand(3, 4, 4)\n",
    "flatten_test_y = flatten(flatten_test_x)\n",
    "assert np.allclose(flatten_test_y, flatten_test_y), \"Flatten function is incorrect\"\n",
    "\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the conv function?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the fc function?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the relu function?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the pool2 function?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=str\n",
    ")\n",
    "answer(\n",
    "    question=\"2.1\",\n",
    "    subquestion=\"What is your implementation of the flatten function?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test inference using the weights of our trained network and our own\n",
    "implementation of the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the previous section\n",
    "PATH = './my_mnist_net.pth'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, padding = 2)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 8 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=False))\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "images_array = torch.zeros((10000,28,28))\n",
    "labels_array = torch.zeros(10000)\n",
    "for i, data in enumerate(torch.utils.data.DataLoader(testset, batch_size=1), 0):\n",
    "    image, label = data\n",
    "    images_array[i,:,:] = image\n",
    "    labels_array[i] = label\n",
    "images_array = images_array.numpy()\n",
    "labels_array = labels_array.numpy()\n",
    "labels_array = labels_array.astype(int)\n",
    "\n",
    "def run_inference(image, net):\n",
    "    # Your code for defining the correct network topology here, using the saved model parameters.\n",
    "    # The first layer is filled in for you.\n",
    "    l1 = conv(image, net.conv1.weight.detach().numpy(), net.conv1.bias.detach().numpy())\n",
    "    #Your Answer Here\n",
    "    output_class = np.argmax(l9)\n",
    "    return output_class\n",
    "\n",
    "def run_inference_net(image, net):\n",
    "    with torch.no_grad():\n",
    "        image = torch.tensor(image).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "        output = net(image)\n",
    "        output_class = torch.argmax(output, dim=1).item()\n",
    "    return output_class\n",
    "    \n",
    "correct, correct_baseline, total = 0, 0, 1000\n",
    "for n in tqdm(range(min(total, int(images_array.shape[0])))):\n",
    "    inference = run_inference(images_array[n], net)\n",
    "    if labels_array[n] == inference:\n",
    "        correct += 1\n",
    "    inference = run_inference_net(images_array[n], net)\n",
    "    if labels_array[n] == inference:\n",
    "        correct_baseline += 1\n",
    "\n",
    "print(f\"Accuracy: {correct}/{total} ({float(correct)/total:.2g})\")\n",
    "print(f\"Baseline Accuracy: {correct_baseline}/{total} ({float(correct_baseline)/total:.2g})\")\n",
    "\n",
    "answer(\n",
    "    \"2.2\",\n",
    "    subquestion=\"What is the accuracy of your implementation?\",\n",
    "    answer= 'FILL ME',\n",
    "    required_type=int\n",
    ")\n",
    "if correct != correct_baseline:\n",
    "    print(f'The accuracy of the implementation is not the same as the baseline accuracy. There is likely a bug in the implementation.')\n",
    "else:\n",
    "    print(f'The accuracy of the implementation is the same as the baseline accuracy. Your implementation is correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to implement the layers of the network, let's analyze some\n",
    "important properties of the network.\n",
    "\n",
    "Now we'll load the network from the previous section and analyze it. Fill a description of the network by filling in the code below. The first and last layers are filled in for you. Include:\n",
    "1. Convolutional layers: Specify a 4-tuple `(weight_width, weight_height, input_channels, filter_count)` (remember that there is one filter for each output channel)\n",
    "2. Fully connected layers: Specify a 2-tuple `(num_output_nodes, num_input_nodes)`\n",
    "3. Pool layer: Specify `(x_window, y_window, x_stride, y_stride)`\n",
    "4. ReLU: No parameters to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_description(weight_width, weight_height, input_channels, filter_count):\n",
    "    return \"conv\", (weight_width, weight_height, input_channels, filter_count)\n",
    "\n",
    "def relu_description():\n",
    "    return \"relu\", ()\n",
    "\n",
    "def pool_description(x_window, y_window, x_stride, y_stride):\n",
    "    return \"pool\", (x_window, y_window, x_stride, y_stride)\n",
    "\n",
    "def fc_description(num_output_nodes, num_input_nodes):\n",
    "    return \"fc\", (num_output_nodes, num_input_nodes)\n",
    "\n",
    "layer_type = [\n",
    "    conv_description(5, 5, 1, 4),\n",
    "    #Your Answer Here\n",
    "    fc_description(10, 256)\n",
    "]\n",
    "\n",
    "network_input_batch = \\\n",
    "    #Your Answer Here\n",
    "network_input_width = \\\n",
    "    #Your Answer Here\n",
    "network_input_height = \\\n",
    "    #Your Answer Here\n",
    "network_input_channels = \\\n",
    "    #Your Answer Here\n",
    "\n",
    "network_input_size = (\n",
    "    network_input_batch,\n",
    "    network_input_channels,\n",
    "    network_input_width,\n",
    "    network_input_height\n",
    ")\n",
    "\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    answer( \n",
    "        question='2.3',\n",
    "        subquestion=f'What is the layer type of layer {index + 1}?',\n",
    "        answer= name,  #Do not Change this Line \n",
    "        required_type=str,\n",
    "    )\n",
    "    answer( \n",
    "        question='2.3',\n",
    "        subquestion=f'What are the parameters of layer {index + 1}?',\n",
    "        answer= param,  #Do not Change this Line\n",
    "        required_type=tuple,\n",
    "    )\n",
    "    \n",
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'What is the size of the input to the network?',\n",
    "    answer= network_input_size,  #Do not Change this Line\n",
    "    required_type=tuple,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of finding the layer input sizes is simply by inspection. Since the inputs of a subsequent layer are the outputs of a previous layer, we can also compute the size of these outputs based on the inputs sizes and weight parameters. Complete the `get_output_size` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(input_sz, layer_type, layer_param):\n",
    "    # Return format: (batch_size, width, height, channels)\n",
    "    input_batch, input_width, input_height, input_channels = input_sz[0], input_sz[1], input_sz[2], input_sz[3]\n",
    "    # ReLU return is filled for you.\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        stride = 1  # You may assume stride = 1\n",
    "        padding = 2  # You may assume padding = 2\n",
    "\n",
    "        # Your code here   \n",
    "        return (0, 0, 0, 0)\n",
    "    \n",
    "    elif layer_type == 'pool':\n",
    "        x_window, y_window, x_stride, y_stride = layer_param\n",
    "\n",
    "        # Your code here\n",
    "\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "\n",
    "        # Your code here\n",
    "\n",
    "        output_height = 1\n",
    "        output_channels = 1\n",
    "    elif layer_type == 'relu':\n",
    "        output_batch = input_batch\n",
    "        output_width = input_width\n",
    "        output_height = input_height\n",
    "        output_channels = input_channels\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return (output_batch, output_width, output_height, output_channels)\n",
    "\n",
    "in_sz = network_input_size\n",
    "sum_size = 0\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    out_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += np.prod(out_sz)\n",
    "    answer( \n",
    "        question='2.4',\n",
    "        subquestion=f'What is the output size of layer {index + 1}?',\n",
    "        answer= out_sz,  #Do not change this line\n",
    "        required_type=tuple,\n",
    "    )\n",
    "    in_sz = out_sz\n",
    "\n",
    "expected = 37300\n",
    "if sum_size != 37300:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of outputs is correct. Good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, complete the `get_weight_size` and function to calculate the number of\n",
    "weights required in each layer and the memory required for storing the weights.\n",
    "You may ignore the memory required for storing biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_weights(layer_type, layer_param):\n",
    "    # Return format: number_of_weights\n",
    "    # ReLU return is filled for you.\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        # Your code here\n",
    "    elif layer_type == 'pool':\n",
    "        # Your code here\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "        # Your code here\n",
    "    elif layer_type == 'relu':\n",
    "        number_of_weights = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return number_of_weights\n",
    "\n",
    "sum_size = 0\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_weights = get_num_weights(name, param)\n",
    "    sum_size += n_weights\n",
    "    answer( \n",
    "        question='2.5',\n",
    "        subquestion=f'How many weights are there in layer {index + 1}?',\n",
    "        answer= n_weights,  # Do not change this line\n",
    "        required_type=Number,\n",
    "    )\n",
    "    \n",
    "expected = 103812\n",
    "if sum_size != 103812:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of weights is correct. Good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of multiplications required per _batch_. Multiplications by\n",
    "zero padding in convolutional layers should still be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_mults(input_sz, layer_type, layer_param):\n",
    "    # Return format: number_of_multiplications\n",
    "    # ReLU return is filled for you.\n",
    "    input_batch, input_width, input_height, input_channels = input_sz[0], input_sz[1], input_sz[2], input_sz[3]\n",
    "    output_batch, output_width, output_height, output_channels = get_output_size(input_sz, layer_type, layer_param)\n",
    "    if layer_type == 'conv':\n",
    "        weight_width, weight_height, input_channels, filter_count = layer_param\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'pool':\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'fc':\n",
    "        num_output_nodes, num_input_nodes = layer_param\n",
    "        num_mult = \\\n",
    "            # Your code here\n",
    "    elif layer_type == 'relu':\n",
    "        num_mult = \\\n",
    "            0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "    return num_mult\n",
    "\n",
    "sum_size = 0\n",
    "in_sz = network_input_size\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_mult = get_num_mults(in_sz, name, param)\n",
    "    in_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += n_mult\n",
    "    answer( \n",
    "        question='2.6',\n",
    "        subquestion=f'How many multiplications are there in layer {index + 1}?',\n",
    "        answer= n_mult,\n",
    "        required_type=Number,\n",
    "    )\n",
    "\n",
    "expected = 5285600\n",
    "if sum_size != 5285600:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total number of multiplications is correct. Good job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_intensity(input_sz, layer_type, layer_param):\n",
    "    # Assume inputs, weights, and outputs are all read/written from main memory.\n",
    "    # Assume one multiply = one compute\n",
    "    # How many computations are done per value read/written?\n",
    "     return 1\n",
    "\n",
    "# Your code here\n",
    "\n",
    "sum_size = 0\n",
    "in_sz = network_input_size\n",
    "for index, (name, param) in enumerate(layer_type):\n",
    "    n_mult = get_compute_intensity(in_sz, name, param)\n",
    "    in_sz = get_output_size(in_sz, name, param)\n",
    "    sum_size += n_mult\n",
    "    answer( \n",
    "        question='2.7',\n",
    "        subquestion=f'what is the compute intensity of layer {index + 1}?',\n",
    "        answer= n_mult,\n",
    "        required_type=Number,\n",
    "    )\n",
    "\n",
    "expected = 54.46173651101803\n",
    "if abs(sum_size - expected) / expected > 0.01:\n",
    "    print(f'Warning! There is a bug in your answer. Expected {expected} but got {sum_size}.')\n",
    "else:\n",
    "    print(f'Total compute intensity is correct. Good job!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
